{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from models.res_unet_adrian import UNet as unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "batch = torch.zeros([batch_size, 3, 80, 80], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 80, 80])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = unet(in_c=3, n_classes=n_classes, layers=[8,16], conv_bridge=True, shortcut=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 2, 80, 80]), torch.float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = model(batch)\n",
    "logits.shape, logits.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 80, 80]), torch.float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = torch.randn([batch_size, 80,80]).ge(0).float()\n",
    "labels.shape, labels.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss = criterion(logits, labels.unsqueeze(dim=1).float())  # BCEWithLogitsLoss()\n",
    "loss = criterion(logits, labels.long())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7731, grad_fn=<NllLoss2DBackward>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dice loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "input_soft = F.softmax(logits, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(labels, n_classes, eps=1e-6):\n",
    "    shape = labels.shape\n",
    "    one_hot = torch.zeros(shape[0], n_classes, *shape[1:])\n",
    "    return one_hot.scatter_(1, labels.unsqueeze(1), 1.0) + eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 80, 80])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 2, 80, 80])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_one_hot = one_hot(labels.long(), n_classes=2)\n",
    "labels_one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4987, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# compute the actual dice score\n",
    "dims = (1, 2, 3)\n",
    "intersection = torch.sum(input_soft * labels_one_hot, dims)\n",
    "cardinality = torch.sum(input_soft + labels_one_hot, dims)\n",
    "\n",
    "dice_score = 2. * intersection / (cardinality + 1e-6)\n",
    "print(torch.mean(torch.tensor(1.) - dice_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4]), torch.Size([4]))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intersection.shape, cardinality.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N_classes=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = unet(in_c=3, n_classes=n_classes, layers=[8,16], conv_bridge=True, shortcut=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 1, 80, 80]), torch.float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = model(batch)\n",
    "logits.shape, logits.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 80, 80]), torch.float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = torch.randn([batch_size, 80,80]).ge(0).float()\n",
    "labels.shape, labels.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = criterion(logits, labels.unsqueeze(dim=1).float())  # BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7649, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dice loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 80, 80])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "input_soft = torch.sigmoid(logits)\n",
    "input_soft.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(labels, n_classes, eps=1e-6):\n",
    "    shape = labels.shape\n",
    "    one_hot = torch.zeros(shape[0], n_classes, *shape[1:])\n",
    "    return one_hot.scatter_(1, labels.unsqueeze(1), 1.0) + eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 80, 80])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4619, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "dims = (1, 2, 3)\n",
    "intersection = torch.sum(input_soft * labels.unsqueeze(dim=1), dims)\n",
    "cardinality = torch.sum(input_soft + labels.unsqueeze(dim=1), dims)\n",
    "dice_score = 2. * intersection / (cardinality + 1e-6)\n",
    "print(torch.mean(torch.tensor(1.) - dice_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4]), torch.Size([4]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intersection.shape, cardinality.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the actual dice score\n",
    "dims = (1, 2, 3)\n",
    "intersection = torch.sum(input_soft * labels_one_hot, dims)\n",
    "cardinality = torch.sum(input_soft + labels_one_hot, dims)\n",
    "\n",
    "dice_score = 2. * intersection / (cardinality + 1e-6)\n",
    "print(torch.mean(torch.tensor(1.) - dice_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from PIL import Image\n",
    "import os, sys\n",
    "import os.path as osp\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix, accuracy_score\n",
    "from utils.evaluation import dice_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels_preds(path_to_preds, csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    im_list, mask_list, gt_list = df.im_paths, df.mask_paths, df.gt_paths\n",
    "\n",
    "    all_bin_preds = []\n",
    "    all_preds = []\n",
    "    all_gts = []\n",
    "    for i in range(len(gt_list)):\n",
    "        im_path = im_list[i].rsplit('/', 1)[-1]\n",
    "        pred_path = osp.join(path_to_preds, im_path[:-4] + '.png')\n",
    "        bin_pred_path = osp.join(path_to_preds, im_path[:-4] + '_binary.png')\n",
    "        gt_path = gt_list[i]\n",
    "        mask_path = mask_list[i]\n",
    "\n",
    "        gt = np.array(Image.open(gt_path)).astype(bool)\n",
    "        mask = np.array(Image.open(mask_path).convert('L')).astype(bool)\n",
    "        from skimage import img_as_float\n",
    "\n",
    "        try: pred = img_as_float(np.array(Image.open(pred_path)))\n",
    "        except FileNotFoundError:\n",
    "            sys.exit('---- no predictions found (maybe run first generate_results.py?) ---- ')\n",
    "        # os.remove(pred_path)\n",
    "        bin_pred = np.array(Image.open(bin_pred_path).convert('L')).astype(bool)\n",
    "        gt_flat = gt.ravel()\n",
    "        mask_flat = mask.ravel()\n",
    "        pred_flat = pred.ravel()\n",
    "        bin_pred_flat = bin_pred.ravel()\n",
    "        # do not consider pixels out of the FOV\n",
    "        noFOV_gt = gt_flat[mask_flat == True]\n",
    "        noFOV_pred = pred_flat[mask_flat == True]\n",
    "        noFOV_bin_pred = bin_pred_flat[mask_flat == True]\n",
    "\n",
    "        # accumulate gt pixels and prediction pixels\n",
    "        all_preds.append(noFOV_pred)\n",
    "        all_bin_preds.append(noFOV_bin_pred)\n",
    "        all_gts.append(noFOV_gt)\n",
    "\n",
    "    return np.hstack(all_preds), np.hstack(all_bin_preds), np.hstack(all_gts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutoff_youden(fpr, tpr, thresholds):\n",
    "    optimal_idx = np.argmax(tpr - fpr)\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    return optimal_threshold\n",
    "\n",
    "\n",
    "def cutoff_dice(preds, gts):\n",
    "    dice_scores = []\n",
    "    thresholds = np.linspace(0, 1, 256)\n",
    "    for i in tqdm(range(len(thresholds))):\n",
    "        thresh = thresholds[i]\n",
    "        hard_preds = preds>thresh\n",
    "        dice_scores.append(dice_score(gts, hard_preds))\n",
    "    dices = np.array(dice_scores)\n",
    "    optimal_threshold = thresholds[dices.argmax()]\n",
    "    return optimal_threshold\n",
    "\n",
    "def cutoff_accuracy(preds, gts):\n",
    "    accuracy_scores = []\n",
    "    thresholds = np.linspace(0, 1, 256)\n",
    "    for i in tqdm(range(len(thresholds))):\n",
    "        thresh = thresholds[i]\n",
    "        hard_preds = preds > thresh\n",
    "        accuracy_scores.append(accuracy_score(gts.astype(np.bool), hard_preds.astype(np.bool)))\n",
    "    accuracies = np.array(accuracy_scores)\n",
    "    optimal_threshold = thresholds[accuracies.argmax()]\n",
    "    return optimal_threshold    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_performance(preds, bin_preds, gts, save_path=None, opt_threshold=None,\n",
    "                        cut_off='dice', mode='train', no_auc=False):\n",
    "    if no_auc: global_auc=0\n",
    "    else:\n",
    "        fpr, tpr, thresholds = roc_curve(gts, preds)\n",
    "        global_auc = auc(fpr, tpr)\n",
    "\n",
    "    if save_path is not None and no_auc==False:\n",
    "        fig = plt.figure(figsize=(8, 8))\n",
    "        plt.plot(fpr, tpr, label='ROC curve')\n",
    "        ll = 'AUC = {:4f}'.format(global_auc)\n",
    "        plt.legend([ll], loc='lower right')\n",
    "        fig.tight_layout()\n",
    "        if opt_threshold is None:\n",
    "            if mode=='train':\n",
    "                plt.savefig(osp.join(save_path,'ROC_train.png'))\n",
    "            elif mode=='val':\n",
    "                plt.savefig(osp.join(save_path, 'ROC_val.png'))\n",
    "        else:\n",
    "            plt.savefig(osp.join(save_path, 'ROC_test.png'))\n",
    "\n",
    "    if opt_threshold is None:\n",
    "        if cut_off == 'acc':\n",
    "            # this would be to get accuracy-maximizing threshold\n",
    "            opt_threshold = cutoff_accuracy(preds, gts)\n",
    "        elif cut_off == 'dice':\n",
    "            # this would be to get dice-maximizing threshold\n",
    "            opt_threshold = cutoff_dice(preds, gts)\n",
    "        else:\n",
    "            opt_threshold = cutoff_youden(fpr, tpr, thresholds)\n",
    "    if no_auc: print('Computing Accuracy...')\n",
    "    acc = accuracy_score(gts, preds > opt_threshold)\n",
    "    if no_auc: print('Accuracy computed')\n",
    "    dice = dice_score(gts, preds > opt_threshold)\n",
    "    if no_auc: print('Dice from ROC-based threshold computed')\n",
    "    dice_from_bin = dice_score(gts, bin_preds)\n",
    "    if no_auc: print('Dice from threshold-free binarized computed')\n",
    "\n",
    "    if no_auc:\n",
    "        print('Can\\'t compute conf_mat on full HRF dataset at once')\n",
    "        specificity, sensitivity = 0,0\n",
    "    else:\n",
    "        tn, fp, fn, tp = confusion_matrix(gts, preds > opt_threshold).ravel()\n",
    "        specificity = tn / (tn + fp)\n",
    "        sensitivity = tp / (tp + fn)\n",
    "\n",
    "    return global_auc, acc, dice, dice_from_bin, specificity, sensitivity, opt_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_path = 'experiments/best_DRIVE/'\n",
    "results_path = 'results/'\n",
    "train_dataset = 'DRIVE'\n",
    "test_dataset = 'AUCKLAND_0'\n",
    "if experiment_path is None: raise Exception('must specify experiment_path, sorry')\n",
    "exp_name = experiment_path\n",
    "cut_off = 'dice'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('* Analyzing performance in ' + train_dataset + ' training set -- Obtaining optimal threshold')\n",
    "path_to_preds = osp.join(results_path, train_dataset, exp_name)\n",
    "save_path = osp.join(path_to_preds, 'perf')\n",
    "\n",
    "perf_csv_path = osp.join(save_path, 'training_performance.csv')\n",
    "if osp.exists(perf_csv_path):\n",
    "    global_auc_tr, acc_tr, dice_tr, dice_from_bin_tr,\\\n",
    "    spec_tr, sens_tr, opt_thresh_tr = pd.read_csv(perf_csv_path).values[0]\n",
    "    print('-- Performance in ' + train_dataset + ' training set had been pre-computed, '\n",
    "                                                 'optimal threshold = {:.4f}'.format(opt_thresh_tr))\n",
    "else:\n",
    "    csv_path = osp.join('data', train_dataset, 'train.csv')\n",
    "    if train_dataset == 'HRF': csv_path = osp.join('data', train_dataset, 'train_fullRes.csv')\n",
    "    preds, bin_preds, gts = get_labels_preds(path_to_preds, csv_path = csv_path)\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    global_auc_tr, acc_tr, dice_tr, dice_from_bin_tr,\\\n",
    "    spec_tr, sens_tr, opt_thresh_tr = compute_performance(preds, bin_preds, gts, save_path=save_path,\n",
    "                                                          opt_threshold=None, cut_off=cut_off, mode='train')\n",
    "    perf_df_train = pd.DataFrame({'auc': global_auc_tr,\n",
    "                                'acc': acc_tr,\n",
    "                                'dice/F1': dice_tr,\n",
    "                                'dice/F1_from_bin': dice_from_bin_tr,\n",
    "                                'spec': spec_tr,\n",
    "                                'sens': sens_tr,\n",
    "                                'opt_t': opt_thresh_tr}, index=[0])\n",
    "    perf_df_train.to_csv(perf_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('* Analyzing performance in ' + train_dataset + ' validation set')\n",
    "perf_csv_path = osp.join(save_path, 'validation_performance.csv')\n",
    "if osp.exists(perf_csv_path):\n",
    "    global_auc_vl, acc_vl, dice_vl, dice_from_bin_vl,\\\n",
    "    spec_vl, sens_vl = pd.read_csv(perf_csv_path).values[0]\n",
    "    print('-- Performance in ' + train_dataset + ' validation set had been pre-computed')\n",
    "else:\n",
    "    csv_path = osp.join('data', train_dataset, 'val.csv')\n",
    "    if train_dataset=='HRF': csv_path = osp.join('data', train_dataset, 'val_fullRes.csv')\n",
    "    preds, bin_preds, gts = get_labels_preds(path_to_preds, csv_path = csv_path)\n",
    "    global_auc_vl, acc_vl, dice_vl, dice_from_bin_vl,\\\n",
    "    spec_vl, sens_vl, _ = compute_performance(preds, bin_preds, gts, save_path=save_path,\n",
    "                                                      opt_threshold=opt_thresh_tr, cut_off=cut_off, mode='train')\n",
    "    perf_df_train = pd.DataFrame({'auc': global_auc_vl,\n",
    "                                  'acc': acc_vl,\n",
    "                                  'dice/F1': dice_vl,\n",
    "                                  'dice/F1_from_bin': dice_from_bin_vl,\n",
    "                                  'spec': spec_vl,\n",
    "                                  'sens': sens_vl}, index=[0])\n",
    "    perf_df_train.to_csv(perf_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('* Analyzing performance in ' + test_dataset + ' test set')\n",
    "path_to_preds = osp.join(results_path, test_dataset, exp_name)\n",
    "save_path = osp.join(path_to_preds, 'perf')\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "perf_csv_path = osp.join(save_path, 'test_performance.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_test_csv = osp.join('data', test_dataset, 'test_all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels_preds(path_to_preds, csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    im_list, mask_list, gt_list = df.im_paths, df.mask_paths, df.gt_paths\n",
    "\n",
    "    all_bin_preds = []\n",
    "    all_preds = []\n",
    "    all_gts = []\n",
    "    for i in range(2,len(gt_list)):\n",
    "        im_path = im_list[i].rsplit('/', 1)[-1]\n",
    "        pred_path = osp.join(path_to_preds, im_path[:-4] + '.png')\n",
    "        bin_pred_path = osp.join(path_to_preds, im_path[:-4] + '_binary.png')\n",
    "        gt_path = gt_list[i]\n",
    "        mask_path = mask_list[i]\n",
    "        \n",
    "        gt = np.array(Image.open(gt_path)).astype(bool)\n",
    "        mask = np.array(Image.open(mask_path).convert('L')).astype(bool)\n",
    "        from skimage import img_as_float\n",
    "\n",
    "        try: pred = img_as_float(np.array(Image.open(pred_path)))\n",
    "        except FileNotFoundError:\n",
    "            sys.exit('---- no predictions found (maybe run first generate_results.py?) ---- ')\n",
    "        # os.remove(pred_path)\n",
    "        \n",
    "        bin_pred = np.array(Image.open(bin_pred_path).convert('L')).astype(bool)\n",
    "        return pred, bin_pred, gt, mask\n",
    "        gt_flat = gt.ravel()\n",
    "        mask_flat = mask.ravel()\n",
    "        pred_flat = pred.ravel()\n",
    "        bin_pred_flat = bin_pred.ravel()\n",
    "        # do not consider pixels out of the FOV\n",
    "        noFOV_gt = gt_flat[mask_flat == True]\n",
    "        noFOV_pred = pred_flat[mask_flat == True]\n",
    "        noFOV_bin_pred = bin_pred_flat[mask_flat == True]\n",
    "\n",
    "        # accumulate gt pixels and prediction pixels\n",
    "        all_preds.append(noFOV_pred)\n",
    "        all_bin_preds.append(noFOV_bin_pred)\n",
    "        all_gts.append(noFOV_gt)\n",
    "\n",
    "    return np.hstack(all_preds), np.hstack(all_bin_preds), np.hstack(all_gts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred, bin_pred, gt, mask = get_labels_preds(path_to_preds, csv_path = path_test_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def imshow_pair(im, gdt):\n",
    "    f, ax = plt.subplots(1, 2, figsize=(12,6))\n",
    "    np_im = np.asarray(im)\n",
    "    np_gdt = np.asarray(gdt)\n",
    "    if len(np_im.shape) == 2:\n",
    "        ax[0].imshow(np_im, cmap='gray'),  ax[0].axis('off')\n",
    "    else:\n",
    "        ax[0].imshow(np_im),  ax[0].axis('off')\n",
    "    if len(np_gdt.shape) == 2:\n",
    "        ax[1].imshow(np.asarray(gdt), cmap = 'gray'), ax[1].axis('off')\n",
    "    else:\n",
    "        ax[1].imshow(np.asarray(gdt)), ax[1].axis('off')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow_pair(pred, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow_pair(bin_pred, gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_flat = gt.ravel()\n",
    "mask_flat = mask.ravel()\n",
    "pred_flat = pred.ravel()\n",
    "bin_pred_flat = bin_pred.ravel()\n",
    "# do not consider pixels out of the FOV\n",
    "noFOV_gt = gt_flat[mask_flat == True]\n",
    "noFOV_pred = pred_flat[mask_flat == True]\n",
    "noFOV_bin_pred = bin_pred_flat[mask_flat == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc, confusion_matrix, accuracy_score\n",
    "from utils.evaluation import dice_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dice_score(noFOV_gt, noFOV_bin_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(noFOV_bin_pred[noFOV_gt==1])*2.0 / (np.sum(noFOV_bin_pred) + np.sum(noFOV_gt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(bin_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, bin_preds, gts = get_labels_preds(path_to_preds, csv_path = path_test_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_auc_test, acc_test, dice_test, dice_from_bin_test, spec_test, sens_test, _ = \\\n",
    "    compute_performance(preds, bin_preds, gts, save_path=save_path, opt_threshold=opt_thresh_tr, no_auc=no_auc)\n",
    "perf_df_test = pd.DataFrame({'auc': global_auc_test,\n",
    "                             'acc': acc_test,\n",
    "                             'dice/F1': dice_test,\n",
    "                             'dice/F1_from_bin':dice_from_bin_test,\n",
    "                             'spec': spec_test,\n",
    "                             'sens': sens_test}, index=[0])\n",
    "perf_df_test.to_csv(perf_csv_path, index=False)\n",
    "print('* Done')\n",
    "print('AUC in Train/Val/Test set is {:.4f}/{:.4f}/{:.4f}'.format(global_auc_tr, global_auc_vl, global_auc_test))\n",
    "print('Accuracy in Train/Val/Test set is {:.4f}/{:.4f}/{:.4f}'.format(acc_tr, acc_vl, acc_test))\n",
    "print('Dice/F1 score in Train/Val/Test set is {:.4f}/{:.4f}/{:.4f}'.format(dice_tr, dice_vl, dice_test))\n",
    "print('Dice/F1 score **from binarized** in Train/Val/Test set is {:.4f}/{:.4f}/{:.4f}'.format(\n",
    "       dice_from_bin_tr, dice_from_bin_vl, dice_from_bin_test))\n",
    "print('Specificity in Train/Val/Test set is {:.4f}/{:.4f}/{:.4f}'.format(spec_tr, spec_vl, spec_test))\n",
    "print('Sensitivity in Train/Val/Test set is {:.4f}/{:.4f}/{:.4f}'.format(sens_tr, sens_vl, sens_test))\n",
    "print('ROC curve plots saved to ', save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from PIL import Image\n",
    "import os, sys\n",
    "import os.path as osp\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix, accuracy_score\n",
    "from utils.evaluation import dice_score\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels_preds(path_to_preds, csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    im_list, mask_list, gt_list = df.im_paths, df.mask_paths, df.gt_paths\n",
    "\n",
    "    all_bin_preds = []\n",
    "    all_preds = []\n",
    "    all_gts = []\n",
    "    for i in range(len(gt_list)):\n",
    "        im_path = im_list[i].rsplit('/', 1)[-1]\n",
    "        pred_path = osp.join(path_to_preds, im_path[:-4] + '.png')\n",
    "        bin_pred_path = osp.join(path_to_preds, im_path[:-4] + '_binary.png')\n",
    "        gt_path = gt_list[i]\n",
    "        mask_path = mask_list[i]\n",
    "\n",
    "        gt = np.array(Image.open(gt_path)).astype(bool)\n",
    "        mask = np.array(Image.open(mask_path).convert('L')).astype(bool)\n",
    "        from skimage import img_as_float\n",
    "        try: pred = img_as_float(np.array(Image.open(pred_path)))\n",
    "        except FileNotFoundError:\n",
    "            sys.exit('---- no predictions found (maybe run first generate_results.py?) ---- ')\n",
    "\n",
    "        bin_pred = np.array(Image.open(bin_pred_path).convert('L')).astype(bool)\n",
    "        gt_flat = gt.ravel()\n",
    "        mask_flat = mask.ravel()\n",
    "        pred_flat = pred.ravel()\n",
    "        bin_pred_flat = bin_pred.ravel()\n",
    "        # do not consider pixels out of the FOV\n",
    "        noFOV_gt = gt_flat[mask_flat == True]\n",
    "        noFOV_pred = pred_flat[mask_flat == True]\n",
    "        noFOV_bin_pred = bin_pred_flat[mask_flat == True]\n",
    "\n",
    "        # accumulate gt pixels and prediction pixels\n",
    "        all_preds.append(noFOV_pred)\n",
    "        all_bin_preds.append(noFOV_bin_pred)\n",
    "        all_gts.append(noFOV_gt)\n",
    "\n",
    "    return np.hstack(all_preds), np.hstack(all_bin_preds), np.hstack(all_gts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'deep_vessels'\n",
    "dataset = 'DRIVE'\n",
    "\n",
    "print('* Analyzing performance of '+ method +' in ' + dataset + ' test set')\n",
    "path_to_preds = osp.join('results', dataset, 'experiments', method)\n",
    "csv_name = 'test.csv'\n",
    "\n",
    "path_test_csv = osp.join('data', dataset, csv_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, bin_preds, gts = get_labels_preds(path_to_preds, csv_path = path_test_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(bin_preds, gts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_auc_test, acc_test, dice_test, spec_test, sens_test =  compute_performance(preds, bin_preds, gts)\n",
    "print('* Done')\n",
    "print('AUC in Test set is {:.4f}'.format(global_auc_test))\n",
    "print('Accuracy in Test set is {:.4f}'.format(acc_test))\n",
    "print('Dice/F1 score in Test set is {:.4f}'.format(dice_test))\n",
    "print('Specificity in Test set is {:.4f}'.format(spec_test))\n",
    "print('Sensitivity in Test set is {:.4f}'.format(sens_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = 'experiments/2020-01-15-10:2735/config.cfg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = 'results/'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "dataset = 'DRIVE'\n",
    "binarize = 'otsu'\n",
    "tta = 'from_preds'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config_file is not None:\n",
    "    if not osp.isfile(config_file): raise Exception('non-existent config file')\n",
    "    with open(config_file, 'r') as f:\n",
    "        args = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = args['layers']\n",
    "layers = args['layers'].split('/')\n",
    "layers = list(map(int, layers))\n",
    "n_classes = args['n_classes']\n",
    "conv_bridge = args['conv_bridge']\n",
    "shortcut = args['shortcut']\n",
    "experiment_path = args['experiment_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if experiment_path is None: raise Exception('must specify path to experiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= unet(in_c=3, n_classes=n_classes, layers=layers, conv_bridge=conv_bridge, shortcut=shortcut).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = osp.join(experiment_path, 'model_checkpoint.pth')\n",
    "checkpoint = torch.load(path, map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model, experiment_path, device):\n",
    "    checkpoint_path = osp.join(experiment_path, 'model_checkpoint.pth')\n",
    "    # checkpoint = torch.load(checkpoint_path)\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    return model, checkpoint['stats']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, stats = load_model(model, experiment_path, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os, sys\n",
    "import os.path as osp\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix, accuracy_score\n",
    "from utils.evaluation import dice_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels_preds(path_to_preds, csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    im_list, mask_list, gt_list = df.im_paths, df.mask_paths, df.gt_paths\n",
    "\n",
    "    all_bin_preds = []\n",
    "    all_preds = []\n",
    "    all_gts = []\n",
    "    for i in range(len(gt_list)):\n",
    "        im_path = im_list[i].rsplit('/', 1)[-1]\n",
    "        pred_path = osp.join(path_to_preds, im_path[:-4] + '.png')\n",
    "        bin_pred_path = osp.join(path_to_preds, im_path[:-4] + '_binary.png')\n",
    "        gt_path = gt_list[i]\n",
    "        mask_path = mask_list[i]\n",
    "\n",
    "        gt = np.array(Image.open(gt_path)).astype(bool)\n",
    "        mask = np.array(Image.open(mask_path).convert('L')).astype(bool)\n",
    "        from skimage import img_as_float\n",
    "\n",
    "        try: pred = img_as_float(np.array(Image.open(pred_path)))\n",
    "        except FileNotFoundError:\n",
    "            sys.exit('---- no predictions found (maybe run first generate_results.py?) ---- ')\n",
    "        # os.remove(pred_path)\n",
    "        bin_pred = np.array(Image.open(bin_pred_path).convert('L')).astype(bool)\n",
    "        gt_flat = gt.ravel()\n",
    "        mask_flat = mask.ravel()\n",
    "        pred_flat = pred.ravel()\n",
    "        bin_pred_flat = bin_pred.ravel()\n",
    "        # do not consider pixels out of the FOV\n",
    "        noFOV_gt = gt_flat[mask_flat == True]\n",
    "        noFOV_pred = pred_flat[mask_flat == True]\n",
    "        noFOV_bin_pred = bin_pred_flat[mask_flat == True]\n",
    "\n",
    "        # accumulate gt pixels and prediction pixels\n",
    "        all_preds.append(noFOV_pred)\n",
    "        all_bin_preds.append(noFOV_bin_pred)\n",
    "        all_gts.append(noFOV_gt)\n",
    "\n",
    "    return np.hstack(all_preds), np.hstack(all_bin_preds), np.hstack(all_gts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutoff_youden(fpr, tpr, thresholds):\n",
    "    optimal_idx = np.argmax(tpr - fpr)\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    return optimal_threshold\n",
    "\n",
    "\n",
    "def cutoff_dice(preds, gts):\n",
    "    dice_scores = []\n",
    "    thresholds = np.linspace(0, 1, 256)\n",
    "    for i in tqdm(range(len(thresholds))):\n",
    "        thresh = thresholds[i]\n",
    "        hard_preds = preds>thresh\n",
    "        dice_scores.append(dice_score(gts, hard_preds))\n",
    "    dices = np.array(dice_scores)\n",
    "    optimal_threshold = thresholds[dices.argmax()]\n",
    "    return optimal_threshold\n",
    "\n",
    "def cutoff_accuracy(preds, gts):\n",
    "    accuracy_scores = []\n",
    "    thresholds = np.linspace(0, 1, 256)\n",
    "    for i in tqdm(range(len(thresholds))):\n",
    "        thresh = thresholds[i]\n",
    "        hard_preds = preds > thresh\n",
    "        accuracy_scores.append(accuracy_score(gts.astype(np.bool), hard_preds.astype(np.bool)))\n",
    "    accuracies = np.array(accuracy_scores)\n",
    "    optimal_threshold = thresholds[accuracies.argmax()]\n",
    "    return optimal_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_performance(preds, bin_preds, gts, save_path=None, opt_threshold=None,\n",
    "                        cut_off='dice', mode='train', no_auc=False):\n",
    "    if no_auc: global_auc=0\n",
    "    else:\n",
    "        fpr, tpr, thresholds = roc_curve(gts, preds)\n",
    "        global_auc = auc(fpr, tpr)\n",
    "\n",
    "    if save_path is not None and no_auc==False:\n",
    "        fig = plt.figure(figsize=(8, 8))\n",
    "        plt.plot(fpr, tpr, label='ROC curve')\n",
    "        ll = 'AUC = {:4f}'.format(global_auc)\n",
    "        plt.legend([ll], loc='lower right')\n",
    "        fig.tight_layout()\n",
    "        if opt_threshold is None:\n",
    "            if mode=='train':\n",
    "                plt.savefig(osp.join(save_path,'ROC_train.png'))\n",
    "            elif mode=='val':\n",
    "                plt.savefig(osp.join(save_path, 'ROC_val.png'))\n",
    "        else:\n",
    "            plt.savefig(osp.join(save_path, 'ROC_test.png'))\n",
    "\n",
    "    if opt_threshold is None:\n",
    "        if cut_off == 'acc':\n",
    "            # this would be to get accuracy-maximizing threshold\n",
    "            opt_threshold = cutoff_accuracy(preds, gts)\n",
    "        elif cut_off == 'dice':\n",
    "            # this would be to get dice-maximizing threshold\n",
    "            opt_threshold = cutoff_dice(preds, gts)\n",
    "        else:\n",
    "            opt_threshold = cutoff_youden(fpr, tpr, thresholds)\n",
    "    if no_auc: print('Computing Accuracy...')\n",
    "    acc = accuracy_score(gts, preds > opt_threshold)\n",
    "    if no_auc: print('Accuracy computed')\n",
    "    dice = dice_score(gts, preds > opt_threshold)\n",
    "    if no_auc: print('Dice from ROC-based threshold computed')\n",
    "    dice_from_bin = dice_score(gts, bin_preds)\n",
    "    if no_auc: print('Dice from threshold-free binarized computed')\n",
    "\n",
    "    if no_auc:\n",
    "        print('Can\\'t compute conf_mat on full HRF dataset at once')\n",
    "        specificity, sensitivity = 0,0\n",
    "    else:\n",
    "        tn, fp, fn, tp = confusion_matrix(gts, preds > opt_threshold).ravel()\n",
    "        specificity = tn / (tn + fp)\n",
    "        sensitivity = tp / (tp + fn)\n",
    "\n",
    "    return global_auc, acc, dice, dice_from_bin, specificity, sensitivity, opt_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_path = 'experiments/'\n",
    "results_path = 'results/'\n",
    "\n",
    "train_dataset = 'HRF'\n",
    "test_dataset = 'HRF'\n",
    "exp_name = 'experiments/2020-01-14-18:2721/'\n",
    "cut_off = 'dice'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.evaluation import fast_auc as auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_preds = osp.join(results_path, train_dataset, exp_name)\n",
    "save_path = osp.join(path_to_preds, 'perf')\n",
    "\n",
    "perf_csv_path = osp.join(save_path, 'validation_performance.csv')\n",
    "if osp.exists(perf_csv_path):\n",
    "    global_auc_vl, acc_vl, dice_vl, dice_from_bin_vl,\\\n",
    "    spec_vl, sens_vl, opt_thresh_vl = pd.read_csv(perf_csv_path).values[0]\n",
    "    print('-- Performance in validation set had been pre-computed, optimal threshold = {:.4f}'.format(\n",
    "        opt_thresh_vl))\n",
    "else:\n",
    "    csv_path = osp.join('data', train_dataset, 'val.csv')\n",
    "    if train_dataset == 'HRF': csv_path = osp.join('data', train_dataset, 'val_fullRes.csv')\n",
    "    preds, bin_preds, gts = get_labels_preds(path_to_preds, csv_path = csv_path)\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    global_auc_vl, acc_vl, dice_vl, dice_from_bin_vl,\\\n",
    "    spec_vl, sens_vl, opt_thresh_vl = compute_performance(preds, bin_preds, gts, save_path=save_path,\n",
    "                                                          opt_threshold=None, cut_off=cut_off, mode='val')\n",
    "    perf_df_val = pd.DataFrame({'auc': global_auc_vl,\n",
    "                                'acc': acc_vl,\n",
    "                                'dice/F1': dice_vl,\n",
    "                                'dice/F1_from_bin': dice_from_bin_vl,\n",
    "                                'spec': spec_vl,\n",
    "                                'sens': sens_vl,\n",
    "                                'opt_t': opt_thresh_vl}, index=[0])\n",
    "    perf_df_val.to_csv(perf_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('* Analyzing performance in training set')\n",
    "perf_csv_path = osp.join(save_path, 'training_performance.csv')\n",
    "if osp.exists(perf_csv_path):\n",
    "    global_auc_tr, acc_tr, dice_tr, dice_from_bin_tr,\\\n",
    "    spec_tr, sens_tr = pd.read_csv(perf_csv_path).values[0]\n",
    "    print('-- Performance in DRIVE training set had been pre-computed')\n",
    "else:\n",
    "    csv_path = osp.join('data', train_dataset, 'train.csv')\n",
    "    if train_dataset=='HRF': csv_path = osp.join('data', train_dataset, 'train_fullRes.csv')\n",
    "    preds, bin_preds, gts = get_labels_preds(path_to_preds, csv_path = csv_path)\n",
    "    global_auc_tr, acc_tr, dice_tr, dice_from_bin_tr,\\\n",
    "    spec_tr, sens_tr, _ = compute_performance(preds, bin_preds, gts, save_path=save_path,\n",
    "                                                      opt_threshold=opt_thresh_vl, cut_off=cut_off, mode='train')\n",
    "    perf_df_train = pd.DataFrame({'auc': global_auc_tr,\n",
    "                                  'acc': acc_tr,\n",
    "                                  'dice/F1': dice_tr,\n",
    "                                  'dice/F1_from_bin': dice_from_bin_tr,\n",
    "                                  'spec': spec_tr,\n",
    "                                  'sens': sens_tr}, index=[0])\n",
    "    perf_df_train.to_csv(perf_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('* Analyzing performance in ' + test_dataset + ' test set')\n",
    "path_to_preds = osp.join(results_path, test_dataset, exp_name)\n",
    "save_path = osp.join(path_to_preds, 'perf')\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "perf_csv_path = osp.join(save_path, 'test_performance.csv')\n",
    "\n",
    "csv_name = 'test.csv' if train_dataset==test_dataset else 'test_all.csv'\n",
    "if test_dataset=='HRF' and csv_name=='test_all.csv':\n",
    "    print(8 * '-', ' For the entire HRF as a test set this can be very memory/time-consuming \\\n",
    "                     --> won\\'t compute ROC curve//spec/sens, sorry', 8 * '-')\n",
    "    no_auc=True\n",
    "else: no_auc=False\n",
    "path_test_csv = osp.join('data', test_dataset, csv_name)\n",
    "preds, bin_preds, gts = get_labels_preds(path_to_preds, csv_path = path_test_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimport matplotlib.pyplot as plt\n",
    "\n",
    "def imshow_pair(im, gdt):\n",
    "    f, ax = plt.subplots(1, 2, figsize=(12,6))\n",
    "    np_im = np.asarray(im)\n",
    "    np_gdt = np.asarray(gdt)\n",
    "    if len(np_im.shape) == 2:\n",
    "        ax[0].imshow(np_im, cmap='gray'),  ax[0].axis('off')\n",
    "    else:\n",
    "        ax[0].imshow(np_im),  ax[0].axis('off')\n",
    "    if len(np_gdt.shape) == 2:\n",
    "        ax[1].imshow(np.asarray(gdt), cmap = 'gray'), ax[1].axis('off')\n",
    "    else:\n",
    "        ax[1].imshow(np.asarray(gdt)), ax[1].axis('off')\n",
    "    plt.tight_layout()learn.metrics import roc_curve\n",
    "fpr, tpr, thresholds = roc_curve(gts, preds)\n",
    "plt.plot(fpr, tpr, label='ROC curve')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import auc\n",
    "auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.evaluation import fast_auc as auc\n",
    "auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sets = ['DRIVE', 'CHASEDB', 'HRF', 'STARE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xxx = data_sets.remove('DRIVE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def imshow_pair(im, gdt):\n",
    "    f, ax = plt.subplots(1, 2, figsize=(12,6))\n",
    "    np_im = np.asarray(im)\n",
    "    np_gdt = np.asarray(gdt)\n",
    "    if len(np_im.shape) == 2:\n",
    "        ax[0].imshow(np_im, cmap='gray'),  ax[0].axis('off')\n",
    "    else:\n",
    "        ax[0].imshow(np_im),  ax[0].axis('off')\n",
    "    if len(np_gdt.shape) == 2:\n",
    "        ax[1].imshow(np.asarray(gdt), cmap = 'gray'), ax[1].axis('off')\n",
    "    else:\n",
    "        ax[1].imshow(np.asarray(gdt)), ax[1].axis('off')\n",
    "    plt.tight_layout()# process HRF data, generate CSVs\n",
    "path_ims = 'data/HRF/images'\n",
    "path_masks = 'data/HRF/mask'\n",
    "path_gts = 'data/HRF/manual1'\n",
    "\n",
    "path_ims_resized = 'data/HRF/images_resized'\n",
    "os.makedirs(path_ims_resized, exist_ok=True)\n",
    "path_masks_resized = 'data/HRF/mask_resized'\n",
    "os.makedirs(path_masks_resized, exist_ok=True)\n",
    "path_gts_resized = 'data/HRF/manual1_resized'\n",
    "os.makedirs(path_gts_resized, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_im_names = sorted(os.listdir(path_ims))\n",
    "all_mask_names = sorted(os.listdir(path_masks))\n",
    "all_gt_names = sorted(os.listdir(path_gts))\n",
    "\n",
    "# append paths\n",
    "num_ims = len(all_im_names)\n",
    "all_im_names = [osp.join(path_ims, n) for n in all_im_names]\n",
    "all_mask_names = [osp.join(path_masks, n) for n in all_mask_names]\n",
    "all_gt_names = [osp.join(path_gts, n) for n in all_gt_names]\n",
    "\n",
    "df_hrf_all = pd.DataFrame({'im_paths': all_im_names,\n",
    "                            'gt_paths': all_gt_names,\n",
    "                            'mask_paths': all_mask_names})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_im_names = all_im_names[:num_ims//2]\n",
    "train_im_names = all_im_names[num_ims//2:]\n",
    "\n",
    "test_mask_names = all_mask_names[:num_ims//2]\n",
    "train_mask_names = all_mask_names[num_ims//2:]\n",
    "\n",
    "test_gt_names = all_gt_names[:num_ims//2]\n",
    "train_gt_names = all_gt_names[num_ims//2:]\n",
    "\n",
    "# use smaller images for trainining on HRF\n",
    "train_im_names_resized = [n.replace(path_ims, path_ims_resized) for n in train_im_names]\n",
    "train_mask_names_resized = [n.replace(path_masks, path_masks_resized) for n in train_mask_names]\n",
    "train_gt_names_resized = [n.replace(path_gts, path_gts_resized) for n in train_gt_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hrf_train_fullRes = pd.DataFrame({'im_paths': train_im_names,\n",
    "                             'gt_paths': train_gt_names,\n",
    "                             'mask_paths': train_mask_names})\n",
    "\n",
    "df_hrf_test = pd.DataFrame({'im_paths': test_im_names,\n",
    "                              'gt_paths': test_gt_names,\n",
    "                               'mask_paths': test_mask_names})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hrf_train = pd.DataFrame({'im_paths': train_im_names_resized,\n",
    "                             'gt_paths': train_gt_names_resized,\n",
    "                             'mask_paths': train_mask_names_resized})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_ims = len(df_hrf_train)\n",
    "tr_ims = int(0.8*num_ims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hrf_train_fullRes, df_hrf_val_fullRes = df_hrf_train_fullRes[:tr_ims], df_hrf_train_fullRes[tr_ims:]\n",
    "df_hrf_train, df_hrf_val = df_hrf_train[:tr_ims], df_hrf_train[tr_ims:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hrf_train.shape, df_hrf_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hrf_train.to_csv('data/HRF/train.csv', index=False)\n",
    "df_hrf_val.to_csv('data/HRF/val.csv', index=False)\n",
    "df_hrf_test.to_csv('data/HRF/test.csv', index=False)\n",
    "df_hrf_all.to_csv('data/HRF/test_all.csv', index=False)\n",
    "\n",
    "df_hrf_train_fullRes.to_csv('data/HRF/train_fullRes.csv', index=False)\n",
    "df_hrf_val_fullRes.to_csv('data/HRF/val_fullRes.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def imshow_pair(im, gdt):\n",
    "    f, ax = plt.subplots(1, 2, figsize=(12,6))\n",
    "    np_im = np.asarray(im)\n",
    "    np_gdt = np.asarray(gdt)\n",
    "    if len(np_im.shape) == 2:\n",
    "        ax[0].imshow(np_im, cmap='gray'),  ax[0].axis('off')\n",
    "    else:\n",
    "        ax[0].imshow(np_im),  ax[0].axis('off')\n",
    "    if len(np_gdt.shape) == 2:\n",
    "        ax[1].imshow(np.asarray(gdt), cmap = 'gray'), ax[1].axis('off')\n",
    "    else:\n",
    "        ax[1].imshow(np.asarray(gdt)), ax[1].axis('off')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.io import imread, imsave\n",
    "from skimage.transform import resize\n",
    "from skimage import img_as_ubyte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Resizing HRF images (**only** for training)\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_name = train_gt_names[i]\n",
    "gt = imread(gt_name)\n",
    "gt_res = resize(gt, (gt.shape[0]//4, gt.shape[1]//4), order=0)\n",
    "print(len(np.unique(gt)), len(np.unique(gt_res)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_name = train_gt_names[i]\n",
    "gt = Image.open(gt_name)\n",
    "gt_res = resize(gt, size=(gt.size[1]//4, gt.size[0]//4), interpolation=Image.NEAREST)\n",
    "gt_res.save('what.png')\n",
    "print(len(np.unique(gt)), len(np.unique(gt_res)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms.functional import resize\n",
    "\n",
    "for i in tqdm(range(len(train_im_names))):\n",
    "    im_name = train_im_names[i]\n",
    "    im_name_out = train_im_names_resized[i]\n",
    "    im = Image.open(im_name)\n",
    "    im_res = resize(im, size=(im.size[1]//4, im.size[0]//4), interpolation=Image.NEAREST)\n",
    "    im_res.save(im_name_out)\n",
    "\n",
    "    mask_name = train_mask_names[i]\n",
    "    mask_name_out = train_mask_names_resized[i]\n",
    "    mask = Image.open(mask_name)\n",
    "    mask_res = resize(mask, size=(mask.size[1]//4, mask.size[0]//4), interpolation=Image.NEAREST)\n",
    "    mask_res.save(mask_name_out)\n",
    "\n",
    "    gt_name = train_gt_names[i]\n",
    "    gt_name_out = train_gt_names_resized[i]\n",
    "    gt = Image.open(gt_name)\n",
    "    gt_res = resize(gt, size=(gt.size[1]//4, gt.size[0]//4), interpolation=Image.NEAREST)\n",
    "    gt_res.save(gt_name_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow_pair(gt, gt_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_ims = 'data/CHASEDB/images'\n",
    "path_masks = 'data/CHASEDB/chase-masks'\n",
    "path_gts = 'data/CHASEDB/manual'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_im_names = sorted(os.listdir(path_ims))\n",
    "all_mask_names = sorted(os.listdir(path_masks))\n",
    "all_gt_names = sorted(os.listdir(path_gts))\n",
    "\n",
    "# append paths\n",
    "all_im_names = [osp.join(path_ims, n) for n in all_im_names]\n",
    "all_mask_names = [osp.join(path_masks, n) for n in all_mask_names]\n",
    "all_gt_names = [osp.join(path_gts, n) for n in all_gt_names if '1st' in n]\n",
    "\n",
    "num_ims = len(all_im_names)\n",
    "test_im_names = all_im_names[:num_ims//2]\n",
    "train_im_names = all_im_names[num_ims//2:]\n",
    "\n",
    "test_mask_names = all_mask_names[:num_ims//2]\n",
    "train_mask_names = all_mask_names[num_ims//2:]\n",
    "\n",
    "test_gt_names = all_gt_names[:num_ims//2]\n",
    "train_gt_names = all_gt_names[num_ims//2:]\n",
    "\n",
    "df_chasedb_all = pd.DataFrame({'im_paths': all_im_names,\n",
    "                             'gt_paths': all_gt_names,\n",
    "                             'mask_paths': all_mask_names})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chasedb_train = pd.DataFrame({'im_paths': train_im_names,\n",
    "                              'gt_paths': train_gt_names,\n",
    "                              'mask_paths': train_mask_names})\n",
    "\n",
    "df_chasedb_test = pd.DataFrame({'im_paths': test_im_names,\n",
    "                              'gt_paths': test_gt_names,\n",
    "                              'mask_paths': test_mask_names})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_ims = len(df_chasedb_train)\n",
    "tr_ims = int(0.8*num_ims)\n",
    "df_chasedb_train, df_chasedb_val = df_chasedb_train[:tr_ims], df_chasedb_train[tr_ims:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_chasedb_train.to_csv('data/CHASEDB/train.csv', index=False)\n",
    "df_chasedb_val.to_csv('data/CHASEDB/val.csv', index=False)\n",
    "df_chasedb_test.to_csv('data/CHASEDB/test.csv', index=False)\n",
    "df_chasedb_all.to_csv('data/CHASEDB/test_all.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_ims = 'data/STARE/stare-images'\n",
    "path_masks = 'data/STARE/stare-masks'\n",
    "path_gts = 'data/STARE/labels-ah'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_im_names = sorted(os.listdir(path_ims))\n",
    "all_mask_names = sorted(os.listdir(path_masks))\n",
    "all_gt_names = sorted(os.listdir(path_gts))\n",
    "\n",
    "# append paths\n",
    "all_im_names = [osp.join(path_ims, n) for n in all_im_names]\n",
    "all_mask_names = [osp.join(path_masks, n) for n in all_mask_names]\n",
    "all_gt_names = [osp.join(path_gts, n) for n in all_gt_names]\n",
    "\n",
    "num_ims = len(all_im_names)\n",
    "test_im_names = all_im_names[:num_ims//2]\n",
    "train_im_names = all_im_names[num_ims//2:]\n",
    "\n",
    "test_mask_names = all_mask_names[:num_ims//2]\n",
    "train_mask_names = all_mask_names[num_ims//2:]\n",
    "\n",
    "test_gt_names = all_gt_names[:num_ims//2]\n",
    "train_gt_names = all_gt_names[num_ims//2:]\n",
    "\n",
    "df_stare_all = pd.DataFrame({'im_paths': all_im_names,\n",
    "                             'gt_paths': all_gt_names,\n",
    "                             'mask_paths': all_mask_names})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stare_train = pd.DataFrame({'im_paths': train_im_names,\n",
    "                              'gt_paths': train_gt_names,\n",
    "                              'mask_paths': train_mask_names})\n",
    "\n",
    "df_stare_test = pd.DataFrame({'im_paths': test_im_names,\n",
    "                              'gt_paths': test_gt_names,\n",
    "                              'mask_paths': test_mask_names})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_ims = len(df_stare_train)\n",
    "tr_ims = int(0.8*num_ims)\n",
    "df_stare_train, df_stare_val = df_stare_train[:tr_ims], df_stare_train[tr_ims:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_stare_train.to_csv('data/STARE/train.csv', index=False)\n",
    "df_stare_val.to_csv('data/STARE/val.csv', index=False)\n",
    "df_stare_test.to_csv('data/STARE/test.csv', index=False)\n",
    "df_stare_all.to_csv('data/STARE/test_all.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from utils import paired_transforms_tv04 as p_tr\n",
    "\n",
    "import os.path as osp\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from skimage.measure import regionprops\n",
    "\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, csv_path, transforms=None, label_values=None):\n",
    "        df = pd.read_csv(csv_path)\n",
    "        self.im_list = df.im_paths\n",
    "        self.gt_list = df.gt_paths\n",
    "        self.mask_list = df.mask_paths\n",
    "        self.transforms = transforms\n",
    "        self.label_values = label_values  # for use in label_encoding\n",
    "\n",
    "    def label_encoding(self, gdt):\n",
    "        gdt_gray = np.array(gdt.convert('L'))\n",
    "        classes = np.arange(len(self.label_values))\n",
    "        for i in classes:\n",
    "            gdt_gray[gdt_gray == self.label_values[i]] = classes[i]\n",
    "        return Image.fromarray(gdt_gray)\n",
    "\n",
    "    def crop_to_fov(self, img, target, mask):\n",
    "        minr, minc, maxr, maxc = regionprops(np.array(mask))[0].bbox\n",
    "        im_crop = Image.fromarray(np.array(img)[minr:maxr, minc:maxc])\n",
    "        tg_crop = Image.fromarray(np.array(target)[minr:maxr, minc:maxc])\n",
    "        mask_crop = Image.fromarray(np.array(mask)[minr:maxr, minc:maxc])\n",
    "        return im_crop, tg_crop, mask_crop\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # load image and labels\n",
    "        img = Image.open(self.im_list[index])\n",
    "        target = Image.open(self.gt_list[index])\n",
    "        mask = Image.open(self.mask_list[index]).convert('L')\n",
    "\n",
    "        img, target, mask = self.crop_to_fov(img, target, mask)\n",
    "        return target\n",
    "        target = np.array(self.label_encoding(target))\n",
    "        plt.figure(figsize=(12,12))\n",
    "        plt.imshow(target, cmap='gray')\n",
    "        plt.show()\n",
    "        target[np.array(mask) == 0] = 0\n",
    "        target = Image.fromarray(target)\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.im_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = '../little_unet/data/HRF'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train_csv = osp.join(path_data, 'train.csv')\n",
    "path_val_csv = osp.join(path_data, 'val.csv')\n",
    "\n",
    "train_dataset = TrainDataset(csv_path=path_train_csv, label_values=[0, 255])\n",
    "val_dataset = TrainDataset(csv_path=path_val_csv, label_values=[0, 255])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im1 = Image.open('../little_unet/data/HRF/manual1_resized/08_g.tif').convert('L')\n",
    "np.unique(np.array(im1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.io import imread, imsave\n",
    "from skimage import img_as_ubyte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# im1 = img_as_ubyte(imread('data/HRF/manual1_resized/08_g.tif').astype(bool))\n",
    "im1 = imread('../little_unet/data/HRF/manual1/08_g.tif')\n",
    "np.unique(np.array(im1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im2=train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow_pair(im1,im2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encoding(gdt, label_values=[0, 255]):\n",
    "    gdt_gray = np.array(gdt.convert('L'))\n",
    "    classes = np.arange(len(label_values))\n",
    "    for i in classes:\n",
    "        gdt_gray[gdt_gray == label_values[i]] = classes[i]\n",
    "    return Image.fromarray(gdt_gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im3 = label_encoding(im2)\n",
    "np.unique(np.array(im1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow_pair(im2, im3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vessels",
   "language": "python",
   "name": "build_central"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
